apiVersion: v1
kind: Namespace
metadata:
  name: llama2

---
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llama2
  namespace: llama2
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
  serveConfigV2: |
    applications:
    - name: llama2
      import_path: "ray_serve_llama2:entrypoint"
      runtime_env:
        env_vars:
          MODEL_ID: "NousResearch/Llama-2-13b-chat-hf"
          NEURON_CC_FLAGS: "-O1"
          LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
          NEURON_CORES: "24"
      deployments:
        - name: Llama-2-13b-chat-hf
          autoscaling_config:
            metrics_interval_s: 0.2
            min_replicas: 1
            max_replicas: 1
            look_back_period_s: 2
            downscale_delay_s: 30
            upscale_delay_s: 2
            target_num_ongoing_requests_per_replica: 1
          graceful_shutdown_timeout_s: 5
          ray_actor_options:
            num_cpus: 30  # Adjusted to fit inf2.8xlarge instance
            resources: {"neuron_cores": 2}  # Adjusted to fit inf2.8xlarge instance
            runtime_env:
              env_vars:
                LD_LIBRARY_PATH: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
  rayClusterConfig:
    rayVersion: 2.22.0
    headGroupSpec:
      headService:
        metadata:
          name: llama2
          namespace: llama2
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            - name: head
              image: public.ecr.aws/data-on-eks/ray2.22.0-py310-llama2-13b-neuron:latest
              imagePullPolicy: Always
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              ports:
                - containerPort: 6379
                  name: gcs
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-logs
              resources:
                limits:
                  cpu: 4
                  memory: 20Gi
                requests:
                  cpu: 4
                  memory: 20Gi
              env:
                - name: LD_LIBRARY_PATH
                  value: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
          nodeSelector:
            instanceType: mixed-x86
            provisionerType: Karpenter
            workload: rayhead
          volumes:
            - name: ray-logs
              emptyDir: {}
    workerGroupSpecs:
      - groupName: inf2
        replicas: 1
        minReplicas: 1
        maxReplicas: 1
        rayStartParams: {}
        template:
          spec:
            containers:
              - name: worker
                image: public.ecr.aws/data-on-eks/ray2.22.0-py310-llama2-13b-neuron:latest
                imagePullPolicy: Always
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh", "-c", "ray stop"]
                resources:
                  limits:
                    cpu: "30"  # Adjusted to fit inf2.8xlarge instance
                    memory: "120Gi"  # Adjusted to fit inf2.8xlarge instance
                    aws.amazon.com/neuron: "2"  # Adjusted to fit inf2.8xlarge instance
                  requests:
                    cpu: "30"  # Adjusted to fit inf2.8xlarge instance
                    memory: "120Gi"  # Adjusted to fit inf2.8xlarge instance
                    aws.amazon.com/neuron: "2"  # Adjusted to fit inf2.8xlarge instance
                env:
                  - name: LD_LIBRARY_PATH
                    value: "/home/ray/anaconda3/lib:$LD_LIBRARY_PATH"
            nodeSelector:
              instanceType: inferentia-inf2
              provisionerType: Karpenter
            tolerations:
              - key: "aws.amazon.com/neuron"
                operator: "Exists"
                effect: "NoSchedule"
              - key: "hub.jupyter.org/dedicated"
                operator: "Equal"
                value: "user"
                effect: "NoSchedule"
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama2
  namespace: llama2
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: "/$1"
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /dashboard/(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: llama2
                port:
                  number: 8265
          - path: /serve/(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: llama2
                port:
                  number: 8000
